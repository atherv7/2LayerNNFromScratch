{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import h5py\n",
    "import scipy \n",
    "from PIL import Image \n",
    "from scipy import ndimage \n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method takes in the size of the input, hidden layer, and output to determine the size of the parameters. \n",
    "#The weights are initailized randomly, and the bias vectors are set to 0 \n",
    "def initialize_parameters(n_x, n_h, n_y): \n",
    "    W1 = 0.01*np.random.randn(n_h, n_x) \n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = 0.01*np.random.randn(n_y, n_h)\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function does a matrix multiplication of the weights and the adds the bias vector\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.matmul(W, A) + b \n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function puts the vector values through the sigmoid activation function \n",
    "#so the neural network does not collapse \n",
    "def sigmoid(Z): \n",
    "    A = np.divide(1, np.add(1, np.exp(np.negative(Z))))\n",
    "    cache = Z \n",
    "\n",
    "    return A, cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function puts the vector values through the relu activation function \n",
    "#so the neural network does not collapse  \n",
    "def relu(Z): \n",
    "    A = np.maximum(Z, 0)\n",
    "    cache = Z \n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function does one forward pass through a node (as in multiplies the input by the weights\n",
    "# and then applies the activation function to it)\n",
    "def linear_activation_forward(A_prev, W, b, activation): \n",
    "    if activation == \"sigmoid\": \n",
    "        Z_lin, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z_lin)\n",
    "    elif activation == \"relu\":\n",
    "        Z_lin, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z_lin)\n",
    "\n",
    "    cache = (linear_cache, activation_cache) \n",
    "\n",
    "    return A, cache "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function computes the loss of our prediction \n",
    "def compute_loss(A, Y): \n",
    "    m = Y.shape[1] \n",
    "\n",
    "    loss = -1 * (1/m) * np.sum(np.multiply(Y, np.log(A)) + np.multiply((1-Y), np.log(1-A)))\n",
    "\n",
    "    loss = np.squeeze(loss) \n",
    "\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function computes the gradients for dW and db for the node \n",
    "#also computes the gradient that can be used for the next node's \n",
    "#backward pass \n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dA_prev = np.matmul(np.transpose(W), dZ)\n",
    "    dW = np.matmul(dZ, np.transpose(A_prev))\n",
    "    db = np.matmul(dZ, np.ones((m,1)))\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes the gradient for the relu activation function for the vector \n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    dZ = np.multiply(dA, (Z > 0).astype(int))\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes the gradient for the sigmoid activation function for the vector \n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "\n",
    "    sigmoid_vals, sigmoid_cache = sigmoid(Z)\n",
    "    dZ = np.multiply(dA, np.multiply(sigmoid_vals, (1-sigmoid_vals)))\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function performs one backward pass through a node \n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dA_prev, dW, db = linear_backward(relu_backward(dA, activation_cache), linear_cache) \n",
    "    elif activation == \"sigmoid\":\n",
    "        dA_prev, dW, db = linear_backward(sigmoid_backward(dA, activation_cache), linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function updates the parameters based on the gradients and the learning rate \n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    for param in parameters: \n",
    "      parameters[param] = parameters[param] - (learning_rate * grads['d' + param])\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for plotting the loss graph \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function loads the data that is stored in the file and divides them by feature and response \n",
    "#and also divides some for training and some for testing \n",
    "def load_data(train_file, test_file):\n",
    "    train_dataset = h5py.File(train_file, mode=\"r\")  \n",
    "\n",
    "    train_set_x_orig = np.array(train_dataset['train_set_x'][:])\n",
    "    train_set_y_orig = np.array(train_dataset['train_set_y'][:])\n",
    "    \n",
    "    test_dataset = h5py.File(test_file, mode=\"r\")\n",
    "    \n",
    "    test_set_x_orig = np.array(test_dataset['test_set_x'][:])\n",
    "    test_set_y_orig = np.array(test_dataset['test_set_y'][:])\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) \n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function performs the training for the model (puts the forward and backward propogation function \n",
    "#together) also plots the loss graph\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_loss=False):\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    losses = []                             \n",
    "    m = X.shape[1]                           \n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, 'sigmoid')\n",
    "\n",
    "        loss = compute_loss(A2, Y)\n",
    "\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2)) / m\n",
    "\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'relu')\n",
    "\n",
    "        grads['dW1'] = dW1 \n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "\n",
    "        update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print(\"Loss after iteration {}: {}\".format(i, np.squeeze(loss)))\n",
    "        if print_loss and i % 100 == 0:\n",
    "            losses.append(loss)\n",
    "       \n",
    "    plt.plot(np.squeeze(losses))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function does a forward pass of the neural net (for testing purposes)\n",
    "def two_layer_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "\n",
    "    A1, cache = linear_activation_forward(A, parameters['W1'], parameters['b1'], 'relu')\n",
    "    caches.append(cache)\n",
    "\n",
    "    A2, cache = linear_activation_forward(A1, parameters['W2'], parameters['b2'], 'sigmoid')\n",
    "    caches.append(cache) \n",
    "\n",
    "            \n",
    "    return A2, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to test the training neural network, and get the ouputs \n",
    "#and calculate the accuracy of the neural net for the data \n",
    "def predict(X, y, parameters):\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 \n",
    "    p = np.zeros((1,m))\n",
    "\n",
    "    probas, caches =  two_layer_forward(X, parameters)\n",
    "\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        p[0][i] = 1 if probas[0][i] >= 0.5 else 0\n",
    "\n",
    "    \n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d2829036e2bc88c939bf26174c1338eed948a5b3088bd14378c54b2fc44eaba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
